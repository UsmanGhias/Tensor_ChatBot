{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1ea2c4-61be-4339-bce7-f4ecc74d06c7",
   "metadata": {},
   "source": [
    "# $$ Train ChatBot $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3f60f-b78d-4760-adcd-6a3a2911ca92",
   "metadata": {},
   "source": [
    "## Import Libraries and Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b319cf62-1ecd-46ef-bd87-88d176f7ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7716a152-6c36-468f-8ec5-0479c7f9193c",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "- `numpy` is imported as `np` for numerical operations.\n",
    "- `Sequential` is a model type in Keras that allows you to build a neural network layer by layer.\n",
    "- `Dense` is a layer type for creating densely-connected neural network layers.\n",
    "- `Activation` is used to specify the activation functions for layers.\n",
    "- `Dropout` is used to add dropout regularization to the model.\n",
    "- `SGD` stands for Stochastic Gradient Descent, which is an optimization algorithm used for training the neural network.\n",
    "- `random` is imported for generating random numbers and is used in various parts of the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c26d6c37-2e88-45f1-a25f-063d66945a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for text processing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize a lemmatizer for word normalization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Import JSON and Pickle for data serialization\n",
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f554dcd-8850-4863-ba7e-20e174a5086a",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "- `nltk` is a powerful library for natural language processing. It provides tools for working with text data.\n",
    "- We import the `WordNetLemmatizer` from `nltk.stem`. A lemmatizer is a tool used to reduce words to their base or dictionary form. This is helpful for text analysis as it reduces words to their common form (e.g., \"running\" to \"run\").\n",
    "- `json` is imported to work with JSON data. JSON is a common data format for storing and exchanging structured data.\n",
    "- `pickle` is imported to work with Python object serialization. It allows us to save and load Python objects like variables or models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f51f741b-c32c-4756-b8db-19715c84bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_file = open('intents.json').read()\n",
    "intents = json.loads(intents_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb54f0a-ef57-48ab-afad-9172a131ea9d",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183ca5f-1375-4693-9c0b-a9aff9f740e0",
   "metadata": {},
   "source": [
    "### Tokenization: Preparing Text Data for Analysis\n",
    "\n",
    "In natural language processing (NLP) and machine learning, the data used by models cannot be raw text; it needs to go through a series of pre-processing steps to make it more suitable for analysis. One of the fundamental preprocessing techniques for textual data is \"tokenization.\"\n",
    "\n",
    "**Tokenization** is the process of breaking down sentences or paragraphs into individual words or tokens. These tokens are the basic building blocks that enable machines to understand and work with textual data. In our code, we use tokenization to convert raw text into a more structured format, making it easier for the machine to analyze.\n",
    "\n",
    "Here's why tokenization is important in our project:\n",
    "\n",
    "- In our project, we are working with intents, which are essentially groups of user queries or patterns and corresponding responses. Each intent is associated with a specific category or purpose, like \"greeting,\" \"farewell,\" or \"ordering.\"\n",
    "\n",
    "- To prepare this textual data for analysis, we tokenize each pattern. This means that we break down sentences or user queries into individual words. For example, the sentence \"How are you doing?\" would be tokenized into individual tokens like \"How,\" \"are,\" \"you,\" and \"doing.\"\n",
    "\n",
    "- The tokenized words are then collected into lists, making it easier for us to manage and work with the textual data. For instance, all the tokens from various user queries associated with a particular intent are stored in a list.\n",
    "\n",
    "- Additionally, we maintain a list of classes, which represents the categories or intents. Each intent is associated with specific patterns. This list of classes helps us understand the scope of our project and categorize user queries effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6f9d4b6-b821-420a-a129-88c6c221ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for words, classes, and documents\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "# Define a list of characters to ignore\n",
    "ignore_letters = ['!', '?', ',', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9daba69a-a9c5-48b4-86ab-22b58b399bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hi', 'there'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hey'], 'greeting'), (['Hola'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Nice', 'chatting', 'to', 'you', ',', 'bye'], 'goodbye'), (['Till', 'next', 'time'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Awesome', ',', 'thanks'], 'thanks'), (['Thanks', 'for', 'helping', 'me'], 'thanks'), (['How', 'you', 'could', 'help', 'me', '?'], 'options'), (['What', 'you', 'can', 'do', '?'], 'options'), (['What', 'help', 'you', 'provide', '?'], 'options'), (['How', 'you', 'can', 'be', 'helpful', '?'], 'options'), (['What', 'support', 'is', 'offered'], 'options'), (['How', 'to', 'check', 'Adverse', 'drug', 'reaction', '?'], 'adverse_drug'), (['Open', 'adverse', 'drugs', 'module'], 'adverse_drug'), (['Give', 'me', 'a', 'list', 'of', 'drugs', 'causing', 'adverse', 'behavior'], 'adverse_drug'), (['List', 'all', 'drugs', 'suitable', 'for', 'patient', 'with', 'adverse', 'reaction'], 'adverse_drug'), (['Which', 'drugs', 'dont', 'have', 'adverse', 'reaction', '?'], 'adverse_drug'), (['Open', 'blood', 'pressure', 'module'], 'blood_pressure'), (['Task', 'related', 'to', 'blood', 'pressure'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'entry'], 'blood_pressure'), (['I', 'want', 'to', 'log', 'blood', 'pressure', 'results'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'management'], 'blood_pressure'), (['I', 'want', 'to', 'search', 'for', 'blood', 'pressure', 'result', 'history'], 'blood_pressure_search'), (['Blood', 'pressure', 'for', 'patient'], 'blood_pressure_search'), (['Load', 'patient', 'blood', 'pressure', 'result'], 'blood_pressure_search'), (['Show', 'blood', 'pressure', 'results', 'for', 'patient'], 'blood_pressure_search'), (['Find', 'blood', 'pressure', 'results', 'by', 'ID'], 'blood_pressure_search'), (['Find', 'me', 'a', 'pharmacy'], 'pharmacy_search'), (['Find', 'pharmacy'], 'pharmacy_search'), (['List', 'of', 'pharmacies', 'nearby'], 'pharmacy_search'), (['Locate', 'pharmacy'], 'pharmacy_search'), (['Search', 'pharmacy'], 'pharmacy_search'), (['Lookup', 'for', 'hospital'], 'hospital_search'), (['Searching', 'for', 'hospital', 'to', 'transfer', 'patient'], 'hospital_search'), (['I', 'want', 'to', 'search', 'hospital', 'data'], 'hospital_search'), (['Hospital', 'lookup', 'for', 'patient'], 'hospital_search'), (['Looking', 'up', 'hospital', 'details'], 'hospital_search')]\n"
     ]
    }
   ],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        words.extend(word)        \n",
    "        #add documents in the corpus\n",
    "        documents.append((word, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5f518-5ce1-436a-afba-457581a505cf",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- The code is designed to process a dataset of intents, where each intent represents a specific category or type of user input (e.g., \"greeting,\" \"farewell,\" \"ordering\").\n",
    "- It loops through each intent within the 'intents' dataset.\n",
    "- Inside each intent, it iterates through the 'patterns,' which are examples of user queries or sentences associated with that intent.\n",
    "- For each pattern, it tokenizes the text, which means it breaks down the sentence into individual words. This is important for understanding and analyzing the text because it separates words from one another.\n",
    "- The tokenized words are then added to the 'words' list, which accumulates all the words from all the patterns and intents.\n",
    "- Additionally, the code creates a 'documents' list. This list stores the tokenized words along with their associated intent tags. This is useful for training a machine learning model to understand which intents are associated with which words.\n",
    "- The 'classes' list keeps track of all the intent tags encountered. If a tag is not already in the list, it is added.\n",
    "- The purpose of this code is to prepare and structure textual data for a chatbot or natural language processing application. By tokenizing the text and organizing it into lists, it becomes easier to analyze, classify, and respond to user input effectively.\n",
    "- The final 'print' statement is there to show the 'documents' list, which is a structured representation of the data that will be used to train a chatbot or perform further NLP tasks. This 'documents' list will contain tokenized patterns and their corresponding intent tags, which are crucial for understanding user intent and generating appropriate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fafa0-3a74-4ea8-903f-66af23028f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c46b8eb-e191-4a05-9e18-37e77f7e5f5d",
   "metadata": {},
   "source": [
    "# $$ Lemmatization $$\n",
    " - we are applyi lemmatization to every word in our dataset. By doing so, we remove duplicates and ensure that our model considers different word forms as a unified concept. This step is crucial for making our model more efficient and capable of handling a wide range of user inputs effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "368e7b38-bc79-4843-bccd-7e989ff54e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Usman\n",
      "[nltk_data]     Ghias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05db3e54-6f2e-432f-9297-b1f68b7628af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 documents\n",
      "9 classes ['adverse_drug', 'blood_pressure', 'blood_pressure_search', 'goodbye', 'greeting', 'hospital_search', 'options', 'pharmacy_search', 'thanks']\n",
      "87 unique lemmatized words [\"'s\", 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "# lemmaztize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca29ea9-2baa-4ee5-9b0c-1afeb64cc423",
   "metadata": {},
   "source": [
    "### This code snippet demonstrates the following:\n",
    "\n",
    "- Lemmatizes each word, converts them to lowercase, and removes duplicates. This step further prepares the words for use in a machine learning model.\n",
    "- Sorts the unique intent classes and prints the count.\n",
    "- Counts the number of documents (combinations of patterns and intents), intent classes, and unique lemmatized words.\n",
    "- Serializes and saves the processed words and intent classes to files using Pickle for future use, such as training a chatbot or natural language processing model.\n",
    "\n",
    "The purpose of this code is to finalize the preprocessing of textual data, ensuring that words are in their lemmatized and lowercase form while saving these processed words and intent classes for future use in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20094cab-fcec-409f-9735-aa7f6d40017b",
   "metadata": {},
   "source": [
    "# Create Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "982e11d1-6c10-4e3a-81b4-04bd95422945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is created\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the training data\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    word_patterns = doc[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    \n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# Shuffle the training data\n",
    "random.shuffle(training)\n",
    "\n",
    "# Separate the bag and output rows into NumPy arrays\n",
    "train_x = np.array([item[0] for item in training])\n",
    "train_y = np.array([item[1] for item in training])\n",
    "\n",
    "print(\"Training data is created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccce915-e5d0-41a6-914e-c380b9c169af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb701f51-f46c-4bf2-b96e-91d7574d36b2",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d82c0372-9541-4bf0-87d0-9ce47dde9058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10/10 [==============================] - 2s 6ms/step - loss: 2.1966 - accuracy: 0.1064 \n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 2.0998 - accuracy: 0.3191\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 2.0266 - accuracy: 0.4043\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.9477 - accuracy: 0.3404\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.9538 - accuracy: 0.4468\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.6690 - accuracy: 0.5319\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.6454 - accuracy: 0.5745\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.5526 - accuracy: 0.5532\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.3071 - accuracy: 0.7447\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.3115 - accuracy: 0.6170\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.2147 - accuracy: 0.5745\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.9932 - accuracy: 0.7234\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.8551 - accuracy: 0.7234\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.9915 - accuracy: 0.6383\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6521 - accuracy: 0.8723\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.7935 - accuracy: 0.7872\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.7872\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5418 - accuracy: 0.8511\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6347 - accuracy: 0.7872\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4802 - accuracy: 0.8511\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6033 - accuracy: 0.8298\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3820 - accuracy: 0.9574\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2967 - accuracy: 0.9362\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.9149\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4085 - accuracy: 0.8298\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2152 - accuracy: 0.9787\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3806 - accuracy: 0.9362\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3051 - accuracy: 0.8936\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2968 - accuracy: 0.9149\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2752 - accuracy: 0.9149\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3103 - accuracy: 0.9149\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3044 - accuracy: 0.9362\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2474 - accuracy: 0.9362\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.2130 - accuracy: 0.9362\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2553 - accuracy: 0.8936\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2816 - accuracy: 0.9149\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1795 - accuracy: 0.9787\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2648 - accuracy: 0.9362\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9787\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9787\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1282 - accuracy: 0.9787\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1285 - accuracy: 0.9574\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9787\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1324 - accuracy: 0.9787\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1525 - accuracy: 0.9574\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9787\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1061 - accuracy: 0.9574\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1305 - accuracy: 0.9574\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1242 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.9149\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1611 - accuracy: 0.9362\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1279 - accuracy: 0.9787\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0388 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9787\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0878 - accuracy: 0.9787\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0772 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1356 - accuracy: 0.9574\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0923 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0499 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9787\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0772 - accuracy: 0.9787\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9787\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9787\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0877 - accuracy: 0.9574\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1092 - accuracy: 0.9787\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9787\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0455 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9787\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0382 - accuracy: 0.9787\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.9574\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1021 - accuracy: 0.9787\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.9574\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9362\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9574\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.9787\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0709 - accuracy: 0.9787\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1151 - accuracy: 0.9574\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9787\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0640 - accuracy: 0.9787\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9787\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9574\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0339 - accuracy: 0.9787\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.9787\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1037 - accuracy: 0.9574\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9787\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0529 - accuracy: 0.9787\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9574\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.9787\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9787\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0406 - accuracy: 0.9787\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0511 - accuracy: 0.9787\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.9787\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.9787\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9787\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0829 - accuracy: 0.9787\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0377 - accuracy: 0.9787\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9787\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9787\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9574\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0370 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.9574\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.9787\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9787\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Model is created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usman Ghias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# deep neural networks model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compiling model. Use the recommended optimizer arguments.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Training and saving the model\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print(\"Model is created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b45c19-c1ee-46e6-ba55-4d2b1879374f",
   "metadata": {},
   "source": [
    "**Model Architecture and Training:**\n",
    "\n",
    "Our model is designed as a neural network consisting of three dense layers. Each layer serves a specific purpose in understanding and classifying user inputs:\n",
    "\n",
    "1. **First Layer (128 Neurons):** This initial layer processes the input data and extracts relevant features. With 128 neurons, it can capture a variety of patterns and information.\n",
    "\n",
    "2. **Second Layer (64 Neurons):** The second layer further refines the feature representation with 64 neurons. This reduces the complexity of the data while retaining crucial information.\n",
    "\n",
    "3. **Last Layer (Number of Classes Neurons):** The final layer is tailored to the number of unique intent classes we have. It's where the model makes predictions about which intent best matches the input.\n",
    "\n",
    "**Dropout Layers:**\n",
    "\n",
    "To prevent overfitting, we've introduced dropout layers. These layers randomly deactivate a fraction of neurons during each training iteration, ensuring that the model doesn't become too specialized on the training data. This promotes better generalization to unseen inputs.\n",
    "\n",
    "**Optimizer:**\n",
    "\n",
    "We've employed the Stochastic Gradient Descent (SGD) optimizer for training our model. SGD is a widely used optimization algorithm for updating model weights during training. It helps the model converge to a solution more effectively.\n",
    "\n",
    "**Training:**\n",
    "\n",
    "We feed our preprocessed and tokenized data into the model and train it. In this code, the model is trained over 200 epochs, which means it goes through the entire dataset 200 times. This extensive training allows the model to learn patterns and associations in the data.\n",
    "\n",
    "**Model Saving:**\n",
    "\n",
    "Once the training is complete, we save the trained model using Keras' `model.save(\"chatbot_model.h5\")` function. This saved model can be used for chatbot development or other natural language processing tasks without the need to retrain it from scratch. It preserves the learned patterns and relationships in the data, making it ready for deployment in practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98b73a-e777-4cb2-bbc0-cf94ed02d9e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a1017-ecb0-4a7a-8f19-a852b9b81855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
